# ğŸ¤– Hybrid-RAG-Assistant

A powerful document-based chatbot built using **Hybrid Retrieval-Augmented Generation (RAG)** combining **semantic similarity search (ChromaDB + Ollama Embeddings)** with **keyword matching**, and powered by **Groqâ€™s LLaMA 3** for generating accurate, grounded answers.

---

## ğŸš€ Features

- ğŸ“„ **Supports multi-format knowledge ingestion**: `.txt`, `.pdf`, `.csv`, `.docx`, `.json`
- ğŸ” **Hybrid search strategy**: Combines semantic + keyword-based document retrieval
- ğŸ§  **Groq LLM integration**: Fast, accurate answers using `llama3-8b-8192`
- ğŸ§± **ChromaDB + Ollama Embeddings**: Local vector database for fast retrieval
- ğŸ³ **Dockerized for production**: Run anywhere with one command
- ğŸ“š **Built with Streamlit**: Simple, intuitive web interface

---

## ğŸ› ï¸ Setup Instructions (Without Docker)

### 1. Clone the repo

```bash
git clone https://github.com/your-username/Hybrid-RAG-Assistant.git
cd Hybrid-RAG-Assistant
```

### 2. Create a virtual environment and activate it

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up your environment variables

Create a `.env` file in the root directory:

```env
GROQ_API_KEY=your_groq_api_key_here
```

---

## ğŸ§ª Run the App Locally

```bash
streamlit run main.py
```

Visit [http://localhost:8501](http://localhost:8501) in your browser.

---

## ğŸ³ Docker Setup (Recommended)

### 1. Build Docker image

```bash
docker build -t hybrid-rag-app .
```

### 2. Run the container

```bash
docker run -p 8501:8501 -e GROQ_API_KEY=your_groq_api_key_here hybrid-rag-app
```

> âš ï¸ Ensure Ollama is running on the host system and `nomic-embed-text` model is available.
> You can start it with:
```bash
ollama run nomic-embed-text
```

If youâ€™re on Linux, you can run the container with host networking:

```bash
docker run --network=host -e GROQ_API_KEY=your_groq_api_key_here hybrid-rag-app
```

---

## ğŸ“‚ File Structure

```bash
Hybrid-RAG-Assistant/
â”œâ”€â”€ main.py                      # Main Streamlit app
â”œâ”€â”€ Dockerfile                   # Dockerfile to containerize the app
â”œâ”€â”€ .env                         # Env variables (GROQ API)
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ loader.py                # File loader utilities
â”‚   â””â”€â”€ chunker.py               # Document chunking logic
â””â”€â”€ README.md
```

---

## ğŸ’¡ How It Works

1. **Upload Files** â†’ `.txt`, `.pdf`, `.docx`, `.csv`, `.json`
2. **Indexing** â†’ Files are chunked and embedded using Ollama's `nomic-embed-text`
3. **Hybrid Retrieval** â†’ 
   - Dense vector similarity via ChromaDB
   - Keyword overlap scoring
4. **Answer Generation** â†’ Groq's `llama3-8b-8192` model answers based strictly on the top retrieved content

---

## ğŸ§  Requirements

- Python 3.10+
- [Groq API Key](https://console.groq.com/keys)
- [Ollama](https://ollama.com/) running with `nomic-embed-text` model pulled
- Docker (for containerized deployment)

---

## ğŸ“ License

MIT License â€” free to use, modify and share with attribution.

---

## ğŸ™ Acknowledgements

- [Groq](https://groq.com/)
- [Ollama](https://ollama.com/)
- [LangChain](https://www.langchain.com/)
- [ChromaDB](https://www.trychroma.com/)
- [Streamlit](https://streamlit.io/)

---

## âœ¨ Demo Preview

![screenshot](demo.png)
